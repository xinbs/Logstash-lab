# ğŸ¤– AI é›†æˆè°ƒç”¨æŒ‡å—

**é¢å‘ç¬¬ä¸‰æ–¹ AI çš„ Logstash è§„åˆ™æµ‹è¯•æœåŠ¡è°ƒç”¨æ–‡æ¡£**

æœ¬æ–‡æ¡£ä¸“é—¨ä¸ºç¬¬ä¸‰æ–¹ AI ç³»ç»Ÿæä¾›å®Œæ•´çš„è°ƒç”¨æŒ‡å—ï¼Œå®ç°è‡ªåŠ¨åŒ–çš„ Logstash è§„åˆ™æµ‹è¯•å’ŒéªŒè¯ã€‚

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ - MCP é›†æˆ (æ¨è)

### ğŸ”— **MCP (Model Context Protocol) é…ç½®**

**æœ€ä½³å®è·µ**: ä½¿ç”¨ MCP åè®®å¯ä»¥è®© AI ç›´æ¥è°ƒç”¨ Logstash æµ‹è¯•å·¥å…·ï¼Œæ— éœ€ç¼–å†™å¤æ‚çš„ HTTP è¯·æ±‚ä»£ç ã€‚

#### âœ… **æ”¯æŒçš„ AI å®¢æˆ·ç«¯**
- Cursor
- Claude Desktop  
- æ”¯æŒ MCP åè®®çš„å…¶ä»– AI å·¥å…·

#### ğŸ”§ **å¿«é€Ÿé…ç½®**

**æ­¥éª¤ 1**: æ‰¾åˆ°é…ç½®æ–‡ä»¶
```bash
# Cursor
~/.cursor/mcp.json

# Claude Desktop (macOS)  
~/Library/Application Support/Claude/claude_desktop_config.json

# Claude Desktop (Windows)
%APPDATA%/Claude/claude_desktop_config.json
```

**æ­¥éª¤ 2**: æ·»åŠ é…ç½®ï¼ˆæ¨è URL æ–¹å¼ï¼‰
```json
{
  "mcpServers": {
    "logstash-test": {
      "url": "http://localhost:19001/mcp",
      "description": "Logstash è§„åˆ™æµ‹è¯•å’Œè°ƒè¯•å·¥å…·"
    }
  }
}
```

**æ­¥éª¤ 3**: é‡å¯ AI å®¢æˆ·ç«¯

#### ğŸ¯ **å¯ç”¨å·¥å…· (8ä¸ª)**
1. **upload_pipeline** - ä¸Šä¼ å®Œæ•´ Pipeline é…ç½®æ–‡ä»¶
2. **send_test_log** - å‘é€æµ‹è¯•æ—¥å¿—è¿›è¡Œè§£æ  
3. **get_parsed_results** - è·å–æœ€æ–°è§£æç»“æœ
4. **clear_results** - æ¸…ç©ºå†å²è§£æç»“æœ
5. **get_logstash_logs** - è·å– Logstash è¿è¡Œæ—¥å¿—
6. **test_pipeline_complete_stream** - æ‰§è¡Œå®Œæ•´æµå¼æµ‹è¯•æµç¨‹
7. **get_test_guidance** - è·å–æ™ºèƒ½æµ‹è¯•æŒ‡å¯¼ âœ¨
8. **health_check** - æœåŠ¡å¥åº·æ£€æŸ¥

#### ğŸ’¡ **æ™ºèƒ½æµ‹è¯•æŒ‡å¯¼**

æ–°å¢çš„ `get_test_guidance` å·¥å…·å¯ä»¥ï¼š
- ğŸ¯ **è‡ªåŠ¨åˆ†æåœºæ™¯**ï¼šæ–°å»ºé…ç½®ã€è°ƒè¯•ä¿®å¤ã€æµ‹è¯•éªŒè¯ã€æ€§èƒ½ä¼˜åŒ–
- ğŸ“‹ **æä¾›æ­¥éª¤æŒ‡å¯¼**ï¼šæ ¹æ®ä¸åŒåœºæ™¯æ¨èæœ€ä½³æµ‹è¯•æµç¨‹
- ğŸ” **é…ç½®æ™ºèƒ½åˆ†æ**ï¼šæ£€æµ‹ Grokã€Rubyã€Mutateã€Date æ’ä»¶å¹¶æä¾›å»ºè®®
- ğŸ“Š **æ—¥å¿—æ ¼å¼è¯†åˆ«**ï¼šè‡ªåŠ¨è¯†åˆ« Syslogã€JSON ç­‰æ ¼å¼
- âš ï¸ **å¸¸è§é—®é¢˜é¢„è­¦**ï¼šæå‰æé†’å¯èƒ½çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### ğŸ› ï¸ **å…¼å®¹æ€§é…ç½®**

å¦‚æœ URL æ–¹å¼ä¸å·¥ä½œï¼Œå¯ä»¥ä½¿ç”¨ curl å‘½ä»¤æ–¹å¼ï¼š
```json
{
  "mcpServers": {
    "logstash-test": {
      "command": "curl",
      "args": [
        "-s", "-X", "POST",
        "http://localhost:19001/mcp", 
        "-H", "Content-Type: application/json",
        "-d", "@-"
      ],
      "description": "Logstash è§„åˆ™æµ‹è¯•å’Œè°ƒè¯•å·¥å…·"
    }
  }
}
```

---

## ğŸ“‹ æœåŠ¡æ¦‚è¿°

### ğŸ¯ **æœåŠ¡åŠŸèƒ½**
- **ğŸŒŸ Pipeline æ–‡ä»¶ä¸Šä¼ å’Œè§£æ**ï¼ˆæ¨èï¼‰
- **Logstash Filter è§„åˆ™ç¼–è¾‘å’ŒéªŒè¯**
- **å®æ—¶æ—¥å¿—è§£ææµ‹è¯•**
- **è§£æç»“æœè·å–å’Œåˆ†æ**
- **è°ƒè¯•æ—¥å¿—æŸ¥çœ‹**
- **è‡ªåŠ¨åŒ–æµ‹è¯•å·¥ä½œæµ**

### ğŸŒ **æœåŠ¡åœ°å€**
```
Web æœåŠ¡: http://localhost:19000
MCP æœåŠ¡å™¨: http://localhost:19001
MCP JSON-RPC: POST http://localhost:19001/mcp
SSE æµ‹è¯•é¡µé¢: http://localhost:19001/test
Health Check: GET http://localhost:19001/tools/health_check
```

### ğŸ”‘ **æ ¸å¿ƒç‰¹æ€§**
- âœ… **æ— éœ€è®¤è¯**ï¼šç›´æ¥è°ƒç”¨ API
- âœ… **çƒ­é‡è½½**ï¼šé…ç½®ä¿®æ”¹ 3 ç§’å†…ç”Ÿæ•ˆ
- âœ… **æ™ºèƒ½å¤„ç†**ï¼šè‡ªåŠ¨æ¡ä»¶åˆ¤æ–­æ›¿æ¢
- âœ… **å®æ—¶åé¦ˆ**ï¼šå³æ—¶è·å–è§£æç»“æœ
- âœ… **é”™è¯¯å¤„ç†**ï¼šè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯

---

## ğŸš€ AI è°ƒç”¨å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ **æœåŠ¡å¥åº·æ£€æŸ¥**

åœ¨å¼€å§‹è°ƒç”¨å‰ï¼Œå…ˆæ£€æŸ¥æœåŠ¡çŠ¶æ€ï¼š

```bash
# æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
curl -s http://localhost:19000/get_parsed_results > /dev/null
echo "Service status: $?"  # 0=å¯ç”¨, é0=ä¸å¯ç”¨
```

**AI å®ç°å»ºè®®**ï¼š
```python
import requests

def check_service_health():
    try:
        response = requests.get("http://localhost:19000/get_parsed_results", timeout=5)
        return response.status_code == 200
    except:
        return False
```

### 2ï¸âƒ£ **åŸºæœ¬è°ƒç”¨å·¥ä½œæµ**

#### ğŸŒŸ **æ¨èæ–¹å¼ï¼šMCP æœåŠ¡å™¨ Pipeline æ–‡ä»¶ä¸Šä¼ å·¥ä½œæµ**

```bash
#!/bin/bash
# AI è‡ªåŠ¨åŒ–æµ‹è¯•å·¥ä½œæµ - MCP æœåŠ¡å™¨æ–¹å¼ï¼ˆæ¨èï¼‰

MCP_URL="http://localhost:19001"
WEB_URL="http://localhost:19000"

# 1. åˆ›å»ºå®Œæ•´çš„ pipeline é…ç½®æ–‡ä»¶
cat > /tmp/test_pipeline.conf << 'EOF'
input {
  http {
    port => 15515
    additional_codecs => { "application/json" => "json" }
  }
}

filter {
  if "apache" == [@metadata][type] {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
    mutate {
      rename => { "clientip" => "src_ip" }
      add_field => { "log_type" => "apache_access" }
    }
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@timestamp"
    }
  }
}

output {
  file {
    path => "/data/out/events.ndjson"
    codec => json_lines
  }
}
EOF

# 2. æ¸…ç©ºå†å²æ•°æ®
curl -s -X POST "$BASE_URL/clear_results"

# 3. ä¸Šä¼  pipeline æ–‡ä»¶ï¼ˆä½¿ç”¨ MCP æœåŠ¡å™¨ï¼Œè‡ªåŠ¨æå– filter å¹¶åº”ç”¨ï¼‰
echo "ğŸ“¤ ä¸Šä¼  Pipeline é…ç½®..."
response=$(curl -s -X POST "$MCP_URL/tools/upload_pipeline" -F 'file=@/tmp/test_pipeline.conf')
echo "$response" | jq '.'

if [ "$(echo "$response" | jq -r .success)" = "true" ]; then
  echo "âœ… Pipeline ä¸Šä¼ æˆåŠŸ"
  echo "ğŸ“‹ æå–çš„ filter æ•°é‡: $(echo "$response" | jq -r .extracted_filters)"
else
  echo "âŒ Pipeline ä¸Šä¼ å¤±è´¥: $(echo "$response" | jq -r .error)"
  exit 1
fi

# 4. ç­‰å¾…çƒ­é‡è½½
sleep 3

# 5. å‘é€æµ‹è¯•æ—¥å¿—ï¼ˆä½¿ç”¨ MCP æœåŠ¡å™¨ï¼‰
echo "ğŸ§ª å‘é€æµ‹è¯•æ—¥å¿—..."
test_response=$(curl -s -X POST "$MCP_URL/tools/send_test_log" \
  -H "Content-Type: application/json" \
  -d '{"log_content": "127.0.0.1 - - [25/Dec/2023:10:00:00 +0000] \"GET /index.html HTTP/1.1\" 200 2326"}')

# 6. è·å–è§£æç»“æœ
curl -s "$BASE_URL/get_parsed_results" | jq '.'

# 7. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -f /tmp/test_pipeline.conf
```

#### ğŸ”§ **ä¼ ç»Ÿæ–¹å¼ï¼šç›´æ¥ Filter ç¼–è¾‘å·¥ä½œæµ**

```bash
#!/bin/bash
# AI è‡ªåŠ¨åŒ–æµ‹è¯•å·¥ä½œæµ - ä¼ ç»Ÿæ–¹å¼

BASE_URL="http://localhost:19000"

# 1. æ¸…ç©ºå†å²æ•°æ®
curl -s -X POST "$BASE_URL/clear_results"

# 2. ä¿å­˜ Filter è§„åˆ™ï¼ˆä½¿ç”¨ --data-urlencode é¿å…ç¼–ç é—®é¢˜ï¼‰
curl -s -X POST "$BASE_URL/save_filter" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  --data-urlencode "filter=grok { match => { \"message\" => \"%{COMBINEDAPACHELOG}\" } }"

# 3. ç­‰å¾…çƒ­é‡è½½
sleep 3

# 4. å‘é€æµ‹è¯•æ—¥å¿—
curl -s -X POST "$BASE_URL/test" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  --data-urlencode "logs=127.0.0.1 - - [25/Dec/2023:10:00:00 +0000] \"GET /index.html HTTP/1.1\" 200 2326"

# 5. è·å–è§£æç»“æœ
curl -s "$BASE_URL/get_parsed_results" | jq '.'
```

---

## ğŸŒŠ MCP æœåŠ¡å™¨ APIï¼ˆæ¨èï¼‰

### ğŸ¯ **MCP æœåŠ¡å™¨ä¼˜åŠ¿**

MCP (Model Context Protocol) æœåŠ¡å™¨ä¸“ä¸º AI è°ƒç”¨è®¾è®¡ï¼Œæä¾›æ›´å¼ºå¤§å’Œä¾¿æ·çš„æ¥å£ï¼š

- ğŸš€ **æ–‡ä»¶ä¸Šä¼ æ”¯æŒ**: ç›´æ¥ä¸Šä¼  Pipeline é…ç½®æ–‡ä»¶ï¼Œé¿å…ç¼–ç é—®é¢˜
- ğŸŒŠ **SSE æµå¼åé¦ˆ**: å®æ—¶ç›‘æ§æµ‹è¯•è¿›åº¦ï¼Œé€‚åˆé•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡
- ğŸ›¡ï¸ **å¤šæ ¼å¼æ”¯æŒ**: æ”¯æŒæ–‡ä»¶ä¸Šä¼ ã€è¡¨å•å’Œ JSON ä¸‰ç§è¾“å…¥æ–¹å¼
- âš¡ **æ ‡å‡†åŒ–é”™è¯¯å¤„ç†**: ç»Ÿä¸€çš„é”™è¯¯å“åº”æ ¼å¼
- ğŸ”„ **æ™ºèƒ½è§£æ**: è‡ªåŠ¨æå– filter å—å¹¶æ›¿æ¢æ¡ä»¶åˆ¤æ–­

### ğŸŒŸ **1. MCP Pipeline ä¸Šä¼ æ¥å£**

**ç«¯ç‚¹**: `POST http://localhost:19001/tools/upload_pipeline`

**æ”¯æŒçš„è¾“å…¥æ ¼å¼**:
1. **æ–‡ä»¶ä¸Šä¼ ** (æ¨è): `-F 'file=@config.conf'`
2. **è¡¨å•æ•°æ®**: `-d 'pipeline=...'`  
3. **JSON æ ¼å¼**: `-d '{"pipeline_content": "..."}'`

**AI è°ƒç”¨ç¤ºä¾‹**:

```python
import requests

# æ–¹å¼1ï¼šæ–‡ä»¶ä¸Šä¼ ï¼ˆæœ€æ¨èï¼‰
def upload_pipeline_file(file_path):
    with open(file_path, 'rb') as f:
        response = requests.post(
            'http://localhost:19001/tools/upload_pipeline',
            files={'file': f}
        )
    return response.json()

# æ–¹å¼2ï¼šJSON æ ¼å¼
def upload_pipeline_json(pipeline_content):
    response = requests.post(
        'http://localhost:19001/tools/upload_pipeline',
        json={'pipeline_content': pipeline_content}
    )
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
result = upload_pipeline_file('bomgar.conf')
if result['success']:
    print(f"âœ… ä¸Šä¼ æˆåŠŸ: {result['message']}")
    print(f"ğŸ“‹ æå–çš„ filter æ•°é‡: {result['extracted_filters']}")
    print(f"ğŸ” é…ç½®é¢„è§ˆ: {result['preview'][:100]}...")
else:
    print(f"âŒ ä¸Šä¼ å¤±è´¥: {result['error']}")
```

**å“åº”æ ¼å¼**:
```json
{
  "success": true,
  "message": "Pipeline å·²æˆåŠŸä¸Šä¼ å¹¶åº”ç”¨åˆ°æµ‹è¯•ç¯å¢ƒ",
  "extracted_filters": 1,
  "preview": "if \"test\" == [@metadata][type] { ... }",
  "raw_response": {
    "ok": true,
    "message": "Pipeline å·²æˆåŠŸä¸Šä¼ å¹¶åº”ç”¨åˆ°æµ‹è¯•ç¯å¢ƒ",
    "extracted_filters": 1,
    "applied_filter_preview": "è¯¦ç»†çš„é…ç½®é¢„è§ˆ"
  }
}
```

### ğŸ§ª **2. MCP å‘é€æµ‹è¯•æ—¥å¿—æ¥å£**

**ç«¯ç‚¹**: `POST http://localhost:19001/tools/send_test_log`

**AI è°ƒç”¨ç¤ºä¾‹**:

```python
def send_test_log(log_content, is_json=False):
    response = requests.post(
        'http://localhost:19001/tools/send_test_log',
        json={
            'log_content': log_content,
            'is_json': is_json
        }
    )
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
result = send_test_log('127.0.0.1 - - [25/Dec/2023:10:00:00 +0000] "GET /index.html HTTP/1.1" 200 2326')
if result['success']:
    print(f"âœ… æ—¥å¿—å‘é€æˆåŠŸ")
    print(f"ğŸ“Š æœ€æ–°è§£æç»“æœ: {result['latest_event']}")
else:
    print(f"âŒ å‘é€å¤±è´¥: {result['error']}")
```

### ğŸ“Š **3. MCP è·å–è§£æç»“æœæ¥å£**

**ç«¯ç‚¹**: `GET http://localhost:19001/tools/get_parsed_results`

```python
def get_parsed_results():
    response = requests.get('http://localhost:19001/tools/get_parsed_results')
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
results = get_parsed_results()
print(f"ğŸ“ˆ æ€»è®°å½•æ•°: {results['events_count']}")
for event in results['events']:
    print(f"ğŸ•’ {event['@timestamp']}: {event.get('message', '')[:50]}...")
```

### ğŸ” **4. MCP å¥åº·æ£€æŸ¥æ¥å£**

**ç«¯ç‚¹**: `GET http://localhost:19001/tools/health_check`

```python
def health_check():
    response = requests.get('http://localhost:19001/tools/health_check')
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
health = health_check()
if health['healthy']:
    print(f"âœ… æœåŠ¡å¥åº·: {health['details']}")
else:
    print(f"âŒ æœåŠ¡å¼‚å¸¸: {health['details']}")
```

### ğŸŒŠ **5. SSE æµå¼æµ‹è¯•æ¥å£**

**ç«¯ç‚¹**: `GET http://localhost:19001/sse/test_pipeline_complete`

**é€‚ç”¨åœºæ™¯**: éœ€è¦å®æ—¶ç›‘æ§é•¿æ—¶é—´è¿è¡Œæµ‹è¯•æµç¨‹çš„ AI ç³»ç»Ÿ

```python
import requests
import json

def sse_test_pipeline(pipeline_content, test_logs):
    params = {
        'pipeline_content': pipeline_content,
        'test_logs': json.dumps(test_logs),
        'is_json': 'false',
        'wait_time': '3'
    }
    
    response = requests.get(
        'http://localhost:19001/sse/test_pipeline_complete',
        params=params,
        stream=True,
        headers={'Accept': 'text/event-stream'}
    )
    
    for line in response.iter_lines(decode_unicode=True):
        if line.startswith('data: '):
            try:
                event_data = json.loads(line[6:])  # å»æ‰ 'data: ' å‰ç¼€
                print(f"[{event_data['timestamp']}] {event_data['type']}: {event_data['data']['message']}")
                
                if event_data['type'] in ['complete', 'error']:
                    break
            except json.JSONDecodeError:
                continue

# ä½¿ç”¨ç¤ºä¾‹
sse_test_pipeline(
    pipeline_content="filter { grok { match => { \"message\" => \"%{GREEDYDATA:content}\" } } }",
    test_logs=["test log message"]
)
```

## ğŸ”§ ä¼ ç»Ÿ Web API æ¥å£

ä»¥ä¸‹æ˜¯ä¼ ç»Ÿ Web æœåŠ¡çš„ API æ¥å£ï¼Œä»ç„¶å¯ç”¨ä½†å»ºè®®ä¼˜å…ˆä½¿ç”¨ MCP æœåŠ¡å™¨ï¼š

### ğŸŒŸ **1. Pipeline æ–‡ä»¶ä¸Šä¼ æ¥å£ï¼ˆWeb ç‰ˆæœ¬ï¼‰**

**ç«¯ç‚¹**: `POST /upload_pipeline`

**åŠŸèƒ½**: ä¸Šä¼ å®Œæ•´çš„ Logstash pipeline é…ç½®æ–‡ä»¶ï¼Œç³»ç»Ÿè‡ªåŠ¨æå– filter å—å¹¶åº”ç”¨åˆ°æµ‹è¯•ç¯å¢ƒ

**ä¼˜åŠ¿**:
- âœ… **å®Œå…¨é¿å… URL ç¼–ç é—®é¢˜**: ä¸ä¼šå‡ºç° `+` å·å˜ç©ºæ ¼ç­‰ç¼–ç é—®é¢˜
- âœ… **ä¿æŒåŸå§‹æ ¼å¼**: è‡ªåŠ¨ä¿ç•™æ¢è¡Œç¬¦ã€ç¼©è¿›å’Œæ³¨é‡Š
- âœ… **æ™ºèƒ½è§£æ**: è‡ªåŠ¨è¯†åˆ«å’Œæå– filter å—ï¼Œæ”¯æŒå¤æ‚åµŒå¥—ç»“æ„
- âœ… **åŒé‡æ”¯æŒ**: åŒæ—¶æ”¯æŒæ–‡ä»¶ä¸Šä¼ å’Œæ–‡æœ¬å†…å®¹ç²˜è´´

**AI è°ƒç”¨ç¤ºä¾‹**:
```python
import requests
import tempfile
import os

def upload_pipeline_file(pipeline_content):
    """æ–¹å¼ä¸€ï¼šæ–‡ä»¶ä¸Šä¼ """
    url = "http://localhost:19000/upload_pipeline"
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.conf', delete=False) as f:
        f.write(pipeline_content)
        temp_file = f.name
    
    try:
        with open(temp_file, 'rb') as f:
            files = {'file': ('pipeline.conf', f, 'text/plain')}
            response = requests.post(url, files=files)
        return response.json()
    finally:
        os.unlink(temp_file)

def upload_pipeline_text(pipeline_content):
    """æ–¹å¼äºŒï¼šæ–‡æœ¬å†…å®¹ä¸Šä¼ """
    url = "http://localhost:19000/upload_pipeline"
    data = {"pipeline": pipeline_content}
    
    response = requests.post(
        url, 
        data=data,
        headers={"Content-Type": "application/x-www-form-urlencoded"}
    )
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
pipeline_config = """
input {
  udp {
    port => 18136
    codec => line {}
    add_field => { "[@metadata][type]" => "bomgar" }
  }
}

filter {
  if "bomgar" == [@metadata][type] {
    grok {
      match => { "message" => "<%{POSINT:syslog_pri}>%{POSINT:syslog_ver} %{DATA:syslog_timestamp} %{GREEDYDATA:content}" }
    }
    mutate {
      add_field => { "__source" => "bomgar", "log_type" => "bomgar_session" }
      convert => { "syslog_pri" => "integer", "syslog_ver" => "integer" }
    }
  }
}

output {
  kafka {
    bootstrap_servers => "localhost:9092"
    topic_id => "logs"
  }
}
"""

# æ¨èä½¿ç”¨æ–‡ä»¶ä¸Šä¼ æ–¹å¼
result = upload_pipeline_file(pipeline_config)
print(f"ä¸Šä¼ ç»“æœ: {result['message']}")
print(f"æå–çš„ filter é¢„è§ˆ: {result.get('applied_filter_preview', '')[:100]}...")
```

**å“åº”æ ¼å¼**:
```json
{
  "ok": true,
  "message": "Pipeline å·²æˆåŠŸä¸Šä¼ å¹¶åº”ç”¨åˆ°æµ‹è¯•ç¯å¢ƒ",
  "extracted_filters": 1,
  "applied_filter_preview": "    if \"bomgar\" == [@metadata][type] {\n        grok {\n            match => { \"message\" => \"<%{POSINT:syslog_pri}>%{POSINT:syslog_ver}...\" }\n        }\n        # More filter rules...\n    }"
}
```

### ğŸ› ï¸ **2. ä¿å­˜ Filter è§„åˆ™ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰**

**ç«¯ç‚¹**: `POST /save_filter`

**åŠŸèƒ½**: ä¿å­˜ Logstash filter é…ç½®å¹¶è‡ªåŠ¨é‡è½½

**è¯·æ±‚æ ¼å¼**:
```http
POST /save_filter HTTP/1.1
Host: localhost:19000
Content-Type: application/x-www-form-urlencoded

filter=<FILTER_CONTENT>
```

**AI è°ƒç”¨ç¤ºä¾‹**:
```python
import requests
import urllib.parse

def save_filter(filter_content):
    url = "http://localhost:19000/save_filter"
    data = {"filter": filter_content}
    
    response = requests.post(
        url, 
        data=data,
        headers={"Content-Type": "application/x-www-form-urlencoded"}
    )
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
filter_rule = """
grok {
  match => { "message" => "%{COMBINEDAPACHELOG}" }
}
mutate {
  rename => { "clientip" => "src_ip" }
}
"""

result = save_filter(filter_rule)
print(f"ä¿å­˜ç»“æœ: {result['message']}")
```

**æ™ºèƒ½åŠŸèƒ½**:
- è‡ªåŠ¨å°†ä»»ä½• `if "xxx" == [@metadata][type]` æ›¿æ¢ä¸º `if "test" == [@metadata][type]`
- è‡ªåŠ¨è®¾ç½®å…ƒæ•°æ®ç±»å‹ä¸º "test"
- 3 ç§’å†…è‡ªåŠ¨çƒ­é‡è½½

**å“åº”æ ¼å¼**:
```json
{
  "ok": true,
  "message": "Filter å·²ä¿å­˜å¹¶è‡ªåŠ¨é‡è½½ (å·²è‡ªåŠ¨æ·»åŠ æ¡ä»¶åˆ¤æ–­: if \"test\" == [@metadata][type])"
}
```

### ğŸ“Š **3. å‘é€æµ‹è¯•æ—¥å¿—**

**ç«¯ç‚¹**: `POST /test`

**åŠŸèƒ½**: å‘é€æµ‹è¯•æ—¥å¿—åˆ° Logstash å¹¶è·å–è§£æç»“æœ

**è¯·æ±‚å‚æ•°**:
- `logs`: è¦æµ‹è¯•çš„æ—¥å¿—å†…å®¹ï¼ˆå¿…éœ€ï¼‰
- `is_json`: æ˜¯å¦ä¸º JSON æ ¼å¼ï¼Œå€¼ä¸º "1" è¡¨ç¤ºæ˜¯ï¼ˆå¯é€‰ï¼‰

**AI è°ƒç”¨ç¤ºä¾‹**:
```python
def send_test_log(log_content, is_json=False):
    url = "http://localhost:19000/test"
    data = {
        "logs": log_content
    }
    if is_json:
        data["is_json"] = "1"
    
    response = requests.post(url, data=data)
    return response.json()

# æµ‹è¯•çº¯æ–‡æœ¬æ—¥å¿—
apache_log = '127.0.0.1 - - [25/Dec/2023:10:00:00 +0000] "GET /index.html HTTP/1.1" 200 2326'
result = send_test_log(apache_log)
print(f"è§£æç»“æœ: {result['events']}")

# æµ‹è¯• JSON æ—¥å¿—
json_log = '{"timestamp": "2023-12-25T10:00:00Z", "level": "info", "message": "ç”¨æˆ·ç™»å½•æˆåŠŸ"}'
result = send_test_log(json_log, is_json=True)
```

**å“åº”æ ¼å¼**:
```json
{
  "ok": true,
  "message": "âœ… æ—¥å¿—å‘é€æˆåŠŸ",
  "events": [
    {
      "@timestamp": "2024-12-25T10:00:00.000Z",
      "message": "åŸå§‹æ—¥å¿—å†…å®¹",
      "clientip": "127.0.0.1",
      "verb": "GET",
      "request": "/index.html",
      "src_ip": "127.0.0.1"
    }
  ]
}
```

### ğŸ“ˆ **4. è·å–è§£æç»“æœ**

**ç«¯ç‚¹**: `GET /get_parsed_results`

**åŠŸèƒ½**: è·å–æœ€æ–°çš„è§£æè®°å½•ï¼ˆæœ€å¤š 50 æ¡ï¼‰

**AI è°ƒç”¨ç¤ºä¾‹**:
```python
def get_parsed_results():
    url = "http://localhost:19000/get_parsed_results"
    response = requests.get(url)
    return response.json()

def analyze_results():
    results = get_parsed_results()
    
    if results["ok"]:
        events = results["events"]
        count = results["count"]
        
        print(f"è·å–åˆ° {count} æ¡è§£æè®°å½•")
        
        # åˆ†æè§£æç»“æœ
        for i, event in enumerate(events):
            print(f"è®°å½• {i+1}:")
            print(f"  æ—¶é—´: {event.get('@timestamp')}")
            print(f"  åŸå§‹æ¶ˆæ¯: {event.get('message', '')[:100]}...")
            
            # æ£€æŸ¥è§£æå‡ºçš„å­—æ®µ
            parsed_fields = [k for k in event.keys() if not k.startswith('@') and k != 'message']
            print(f"  è§£æå­—æ®µ: {', '.join(parsed_fields)}")
            
        return events
    else:
        print(f"è·å–å¤±è´¥: {results.get('message')}")
        return []
```

**å“åº”æ ¼å¼**:
```json
{
  "ok": true,
  "events": [
    {
      "@timestamp": "2024-12-25T10:00:00.000Z",
      "message": "åŸå§‹æ—¥å¿—",
      "parsed_field1": "value1",
      "parsed_field2": "value2",
      "_parsed_time": "2024-12-25 10:00:15"
    }
  ],
  "count": 1,
  "message": "æˆåŠŸè·å– 1 æ¡è§£æè®°å½•"
}
```

### ğŸ“‹ **5. è·å– Logstash æ—¥å¿—**

**ç«¯ç‚¹**: `GET /logstash_logs`

**åŠŸèƒ½**: è·å– Logstash è¿è¡Œæ—¥å¿—ï¼Œç”¨äºè°ƒè¯•

**AI è°ƒç”¨ç¤ºä¾‹**:
```python
def get_logstash_logs():
    url = "http://localhost:19000/logstash_logs"
    response = requests.get(url)
    return response.json()

def check_for_errors():
    logs_result = get_logstash_logs()
    
    if logs_result["ok"]:
        logs = logs_result["logs"]
        
        # æ£€æŸ¥é”™è¯¯ä¿¡æ¯
        error_lines = [line for line in logs.split('\n') if 'ERROR' in line.upper()]
        warning_lines = [line for line in logs.split('\n') if 'WARN' in line.upper()]
        
        if error_lines:
            print("å‘ç°é”™è¯¯:")
            for error in error_lines[-5:]:  # æœ€è¿‘ 5 ä¸ªé”™è¯¯
                print(f"  {error}")
                
        if warning_lines:
            print("å‘ç°è­¦å‘Š:")
            for warning in warning_lines[-3:]:  # æœ€è¿‘ 3 ä¸ªè­¦å‘Š
                print(f"  {warning}")
                
        return len(error_lines) == 0  # è¿”å›æ˜¯å¦æ— é”™è¯¯
    
    return False
```

### ğŸ—‘ï¸ **6. æ¸…ç©ºè§£æç»“æœ**

**ç«¯ç‚¹**: `POST /clear_results`

**åŠŸèƒ½**: æ¸…ç©ºå†å²è§£æç»“æœï¼Œé‡æ–°å¼€å§‹æµ‹è¯•

**AI è°ƒç”¨ç¤ºä¾‹**:
```python
def clear_results():
    url = "http://localhost:19000/clear_results"
    response = requests.post(url)
    return response.json()

# åœ¨æ–°æµ‹è¯•å‰æ¸…ç©ºå†å²æ•°æ®
clear_results()
```

---

## ğŸ¤– å®Œæ•´çš„ AI é›†æˆç±»

```python
import requests
import time
import json
from typing import Dict, List, Optional, Union

class LogstashTestService:
    """
    Logstash æµ‹è¯•æœåŠ¡ AI å®¢æˆ·ç«¯
    ä¸“ä¸º AI è‡ªåŠ¨åŒ–è°ƒç”¨è®¾è®¡
    """
    
    def __init__(self, base_url: str = "http://localhost:19000"):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/x-www-form-urlencoded'
        })
    
    def health_check(self) -> bool:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        try:
            response = self.session.get(f"{self.base_url}/get_parsed_results", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def upload_pipeline(self, pipeline_content: str, use_file: bool = True) -> Dict:
        """ä¸Šä¼  Pipeline é…ç½®ï¼ˆæ¨èæ–¹å¼ï¼‰"""
        url = f"{self.base_url}/upload_pipeline"
        
        if use_file:
            # æ–¹å¼ä¸€ï¼šæ–‡ä»¶ä¸Šä¼ ï¼ˆæ¨èï¼‰
            import tempfile
            import os
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.conf', delete=False) as f:
                f.write(pipeline_content)
                temp_file = f.name
            
            try:
                with open(temp_file, 'rb') as f:
                    files = {'file': ('pipeline.conf', f, 'text/plain')}
                    response = requests.post(url, files=files)
                return response.json()
            finally:
                os.unlink(temp_file)
        else:
            # æ–¹å¼äºŒï¼šæ–‡æœ¬å†…å®¹ä¸Šä¼ 
            data = {"pipeline": pipeline_content}
            response = self.session.post(url, data=data)
            return response.json()
    
    def save_filter(self, filter_content: str) -> Dict:
        """ä¿å­˜ Filter è§„åˆ™ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰"""
        data = {"filter": filter_content}
        response = self.session.post(f"{self.base_url}/save_filter", data=data)
        return response.json()
    
    def send_test_log(self, logs: str, is_json: bool = False) -> Dict:
        """å‘é€æµ‹è¯•æ—¥å¿—"""
        data = {"logs": logs}
        if is_json:
            data["is_json"] = "1"
        
        response = self.session.post(f"{self.base_url}/test", data=data)
        return response.json()
    
    def get_parsed_results(self) -> Dict:
        """è·å–è§£æç»“æœ"""
        response = self.session.get(f"{self.base_url}/get_parsed_results")
        return response.json()
    
    def get_logstash_logs(self) -> Dict:
        """è·å– Logstash æ—¥å¿—"""
        response = self.session.get(f"{self.base_url}/logstash_logs")
        return response.json()
    
    def clear_results(self) -> Dict:
        """æ¸…ç©ºè§£æç»“æœ"""
        response = self.session.post(f"{self.base_url}/clear_results")
        return response.json()
    
    def test_pipeline_with_logs(self, pipeline_content: str, test_logs: List[str], 
                               is_json: bool = False, wait_time: int = 3, use_file: bool = True) -> Dict:
        """
        å®Œæ•´çš„ Pipeline æµ‹è¯•å·¥ä½œæµï¼šä¸Šä¼  pipeline -> å‘é€æ—¥å¿— -> è·å–ç»“æœï¼ˆæ¨èï¼‰
        
        Args:
            pipeline_content: å®Œæ•´çš„ Pipeline é…ç½®å†…å®¹
            test_logs: æµ‹è¯•æ—¥å¿—åˆ—è¡¨
            is_json: æ˜¯å¦ä¸º JSON æ ¼å¼æ—¥å¿—
            wait_time: ç­‰å¾…çƒ­é‡è½½æ—¶é—´ï¼ˆç§’ï¼‰
            use_file: æ˜¯å¦ä½¿ç”¨æ–‡ä»¶ä¸Šä¼ æ–¹å¼
        
        Returns:
            åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
        """
        result = {
            "success": False,
            "steps": {},
            "parsed_events": [],
            "errors": []
        }
        
        try:
            # 1. å¥åº·æ£€æŸ¥
            if not self.health_check():
                result["errors"].append("æœåŠ¡ä¸å¯ç”¨")
                return result
            result["steps"]["health_check"] = "âœ… æœåŠ¡å¯ç”¨"
            
            # 2. æ¸…ç©ºå†å²ç»“æœ
            clear_resp = self.clear_results()
            result["steps"]["clear_results"] = clear_resp.get("message", "æ¸…ç©ºå®Œæˆ")
            
            # 3. ä¸Šä¼  Pipeline
            upload_resp = self.upload_pipeline(pipeline_content, use_file)
            if not upload_resp.get("ok"):
                result["errors"].append(f"Pipeline ä¸Šä¼ å¤±è´¥: {upload_resp.get('message')}")
                return result
            result["steps"]["upload_pipeline"] = upload_resp.get("message")
            
            # 4. ç­‰å¾…çƒ­é‡è½½
            time.sleep(wait_time)
            result["steps"]["wait_reload"] = f"ç­‰å¾… {wait_time} ç§’çƒ­é‡è½½"
            
            # 5. å‘é€æµ‹è¯•æ—¥å¿—
            for i, log in enumerate(test_logs):
                send_resp = self.send_test_log(log, is_json)
                if send_resp.get("ok"):
                    result["steps"][f"send_log_{i+1}"] = send_resp.get("message")
                else:
                    result["errors"].append(f"æ—¥å¿— {i+1} å‘é€å¤±è´¥: {send_resp.get('message')}")
            
            # 6. è·å–è§£æç»“æœ
            parsed_resp = self.get_parsed_results()
            if parsed_resp.get("ok"):
                result["parsed_events"] = parsed_resp.get("events", [])
                result["steps"]["get_results"] = f"è·å–åˆ° {len(result['parsed_events'])} æ¡è§£æè®°å½•"
                result["success"] = True
            else:
                result["errors"].append(f"è·å–ç»“æœå¤±è´¥: {parsed_resp.get('message')}")
            
            # 7. æ£€æŸ¥é”™è¯¯æ—¥å¿—
            logs_resp = self.get_logstash_logs()
            if logs_resp.get("ok"):
                logs_content = logs_resp.get("logs", "")
                error_count = logs_content.upper().count("ERROR")
                if error_count > 0:
                    result["errors"].append(f"å‘ç° {error_count} ä¸ª Logstash é”™è¯¯")
                result["steps"]["check_logs"] = f"æ£€æŸ¥æ—¥å¿—å®Œæˆï¼Œé”™è¯¯æ•°: {error_count}"
            
        except Exception as e:
            result["errors"].append(f"æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        
        return result

    def test_filter_with_logs(self, filter_content: str, test_logs: List[str], 
                             is_json: bool = False, wait_time: int = 3) -> Dict:
        """
        å®Œæ•´çš„æµ‹è¯•å·¥ä½œæµï¼šä¿å­˜ filter -> å‘é€æ—¥å¿— -> è·å–ç»“æœ
        
        Args:
            filter_content: Filter è§„åˆ™å†…å®¹
            test_logs: æµ‹è¯•æ—¥å¿—åˆ—è¡¨
            is_json: æ˜¯å¦ä¸º JSON æ ¼å¼æ—¥å¿—
            wait_time: ç­‰å¾…çƒ­é‡è½½æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
        """
        result = {
            "success": False,
            "steps": {},
            "parsed_events": [],
            "errors": []
        }
        
        try:
            # 1. å¥åº·æ£€æŸ¥
            if not self.health_check():
                result["errors"].append("æœåŠ¡ä¸å¯ç”¨")
                return result
            result["steps"]["health_check"] = "âœ… æœåŠ¡å¯ç”¨"
            
            # 2. æ¸…ç©ºå†å²ç»“æœ
            clear_resp = self.clear_results()
            result["steps"]["clear_results"] = clear_resp.get("message", "æ¸…ç©ºå®Œæˆ")
            
            # 3. ä¿å­˜ Filter
            save_resp = self.save_filter(filter_content)
            if not save_resp.get("ok"):
                result["errors"].append(f"Filter ä¿å­˜å¤±è´¥: {save_resp.get('message')}")
                return result
            result["steps"]["save_filter"] = save_resp.get("message")
            
            # 4. ç­‰å¾…çƒ­é‡è½½
            time.sleep(wait_time)
            result["steps"]["wait_reload"] = f"ç­‰å¾… {wait_time} ç§’çƒ­é‡è½½"
            
            # 5. å‘é€æµ‹è¯•æ—¥å¿—
            for i, log in enumerate(test_logs):
                send_resp = self.send_test_log(log, is_json)
                if send_resp.get("ok"):
                    result["steps"][f"send_log_{i+1}"] = send_resp.get("message")
                else:
                    result["errors"].append(f"æ—¥å¿— {i+1} å‘é€å¤±è´¥: {send_resp.get('message')}")
            
            # 6. è·å–è§£æç»“æœ
            parsed_resp = self.get_parsed_results()
            if parsed_resp.get("ok"):
                result["parsed_events"] = parsed_resp.get("events", [])
                result["steps"]["get_results"] = f"è·å–åˆ° {len(result['parsed_events'])} æ¡è§£æè®°å½•"
                result["success"] = True
            else:
                result["errors"].append(f"è·å–ç»“æœå¤±è´¥: {parsed_resp.get('message')}")
            
            # 7. æ£€æŸ¥é”™è¯¯æ—¥å¿—
            logs_resp = self.get_logstash_logs()
            if logs_resp.get("ok"):
                logs_content = logs_resp.get("logs", "")
                error_count = logs_content.upper().count("ERROR")
                if error_count > 0:
                    result["errors"].append(f"å‘ç° {error_count} ä¸ª Logstash é”™è¯¯")
                result["steps"]["check_logs"] = f"æ£€æŸ¥æ—¥å¿—å®Œæˆï¼Œé”™è¯¯æ•°: {error_count}"
            
        except Exception as e:
            result["errors"].append(f"æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        
        return result
    
    def analyze_parsing_effectiveness(self, events: List[Dict]) -> Dict:
        """åˆ†æè§£ææ•ˆæœ"""
        if not events:
            return {"effectiveness": 0, "details": "æ— è§£æç»“æœ"}
        
        analysis = {
            "total_events": len(events),
            "parsed_fields": {},
            "effectiveness": 0,
            "field_coverage": {},
            "details": []
        }
        
        # ç»Ÿè®¡å­—æ®µåˆ†å¸ƒ
        all_fields = set()
        for event in events:
            event_fields = [k for k in event.keys() if not k.startswith('@') and k not in ['message', 'host']]
            all_fields.update(event_fields)
            
            for field in event_fields:
                if field not in analysis["parsed_fields"]:
                    analysis["parsed_fields"][field] = 0
                analysis["parsed_fields"][field] += 1
        
        # è®¡ç®—å­—æ®µè¦†ç›–ç‡
        for field, count in analysis["parsed_fields"].items():
            coverage = (count / len(events)) * 100
            analysis["field_coverage"][field] = f"{coverage:.1f}%"
        
        # è®¡ç®—æ•´ä½“æœ‰æ•ˆæ€§
        if len(all_fields) > 0:
            analysis["effectiveness"] = min(100, len(all_fields) * 10)  # æ¯ä¸ªå­—æ®µè´¡çŒ®10åˆ†ï¼Œæœ€é«˜100åˆ†
        
        # ç”Ÿæˆè¯¦ç»†è¯´æ˜
        analysis["details"] = [
            f"å…±è§£æ {len(events)} æ¡æ—¥å¿—",
            f"æå–å­—æ®µ {len(all_fields)} ä¸ª: {', '.join(sorted(all_fields))}",
            f"è§£ææœ‰æ•ˆæ€§è¯„åˆ†: {analysis['effectiveness']}/100"
        ]
        
        return analysis

# ä½¿ç”¨ç¤ºä¾‹
def ai_test_example():
    """AI è°ƒç”¨ç¤ºä¾‹ - æ¨èä½¿ç”¨ Pipeline æ–¹å¼"""
    service = LogstashTestService()
    
    # ğŸŒŸ æ¨èæ–¹å¼ï¼šä½¿ç”¨å®Œæ•´çš„ Pipeline é…ç½®
    apache_pipeline = """
input {
  http {
    port => 15515
    additional_codecs => { "application/json" => "json" }
  }
}

filter {
  if "apache" == [@metadata][type] {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
    mutate {
      rename => { "clientip" => "src_ip" }
      add_field => { "log_type" => "apache_access" }
    }
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@timestamp"
    }
  }
}

output {
  file {
    path => "/data/out/events.ndjson"
    codec => json_lines
  }
}
    """
    
    apache_logs = [
        '127.0.0.1 - - [25/Dec/2023:10:00:00 +0000] "GET /index.html HTTP/1.1" 200 2326',
        '192.168.1.100 - - [25/Dec/2023:10:00:01 +0000] "POST /api/login HTTP/1.1" 401 128'
    ]
    
    # ğŸŒŸ æ‰§è¡Œ Pipeline æµ‹è¯•ï¼ˆæ¨èï¼‰
    result = service.test_pipeline_with_logs(apache_pipeline, apache_logs)
    
    if result["success"]:
        print("âœ… Pipeline æµ‹è¯•æˆåŠŸ!")
        print("\nğŸ“‹ æ‰§è¡Œæ­¥éª¤:")
        for step, message in result["steps"].items():
            print(f"  {step}: {message}")
        
        print(f"\nğŸ“Š è§£æç»“æœ: {len(result['parsed_events'])} æ¡è®°å½•")
        
        # åˆ†æè§£ææ•ˆæœ
        analysis = service.analyze_parsing_effectiveness(result["parsed_events"])
        print(f"\nğŸ“ˆ è§£æåˆ†æ:")
        for detail in analysis["details"]:
            print(f"  {detail}")
            
        print(f"\nğŸ” å­—æ®µè¦†ç›–ç‡:")
        for field, coverage in analysis["field_coverage"].items():
            print(f"  {field}: {coverage}")
            
    else:
        print("âŒ Pipeline æµ‹è¯•å¤±è´¥!")
        for error in result["errors"]:
            print(f"  é”™è¯¯: {error}")

def ai_test_example_legacy():
    """AI è°ƒç”¨ç¤ºä¾‹ - ä¼ ç»Ÿ Filter æ–¹å¼"""
    service = LogstashTestService()
    
    # ä¼ ç»Ÿæ–¹å¼ï¼šä»…ä½¿ç”¨ Filter è§„åˆ™
    apache_filter = """
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
    mutate {
      rename => { "clientip" => "src_ip" }
      add_field => { "log_type" => "apache" }
    }
    """
    
    apache_logs = [
        '127.0.0.1 - - [25/Dec/2023:10:00:00 +0000] "GET /index.html HTTP/1.1" 200 2326',
        '192.168.1.100 - - [25/Dec/2023:10:00:01 +0000] "POST /api/login HTTP/1.1" 401 128'
    ]
    
    # æ‰§è¡Œä¼ ç»Ÿæµ‹è¯•
    result = service.test_filter_with_logs(apache_filter, apache_logs)
    
    if result["success"]:
        print("âœ… Filter æµ‹è¯•æˆåŠŸ!")
        print(f"\nğŸ“Š è§£æç»“æœ: {len(result['parsed_events'])} æ¡è®°å½•")
        
        # åˆ†æè§£ææ•ˆæœ
        analysis = service.analyze_parsing_effectiveness(result["parsed_events"])
        print(f"\nğŸ“ˆ è§£æåˆ†æ:")
        for detail in analysis["details"]:
            print(f"  {detail}")
    else:
        print("âŒ Filter æµ‹è¯•å¤±è´¥!")
        for error in result["errors"]:
            print(f"  é”™è¯¯: {error}")

if __name__ == "__main__":
    # æ¨èä½¿ç”¨ Pipeline æ–¹å¼
    ai_test_example()
    
    # å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥æµ‹è¯•ä¼ ç»Ÿæ–¹å¼
    # ai_test_example_legacy()
```

---

## ğŸ¯ AI ä½¿ç”¨åœºæ™¯å’Œæ¨¡å¼

### ğŸ“Š **1. æ—¥å¿—æ ¼å¼éªŒè¯**

```python
def validate_log_format(service, log_samples, expected_fields):
    """éªŒè¯æ—¥å¿—æ ¼å¼æ˜¯å¦èƒ½æ­£ç¡®è§£æå‡ºæœŸæœ›å­—æ®µ"""
    
    # ä½¿ç”¨é€šç”¨ grok æ¨¡å¼
    generic_filter = 'grok { match => { "message" => "%{GREEDYDATA:content}" } }'
    
    result = service.test_filter_with_logs(generic_filter, log_samples)
    
    if result["success"]:
        # æ£€æŸ¥æ˜¯å¦è§£æå‡ºæœŸæœ›å­—æ®µ
        events = result["parsed_events"]
        found_fields = set()
        
        for event in events:
            found_fields.update(event.keys())
        
        missing_fields = set(expected_fields) - found_fields
        extra_fields = found_fields - set(expected_fields) - {'@timestamp', 'message', 'host'}
        
        return {
            "valid": len(missing_fields) == 0,
            "found_fields": list(found_fields),
            "missing_fields": list(missing_fields),
            "extra_fields": list(extra_fields)
        }
    
    return {"valid": False, "error": result["errors"]}
```

### ğŸ”§ **2. Filter è§„åˆ™ä¼˜åŒ–**

```python
def optimize_filter_rules(service, base_filter, test_logs, optimization_goals):
    """åŸºäºæµ‹è¯•ç»“æœä¼˜åŒ– Filter è§„åˆ™"""
    
    results = []
    
    # æµ‹è¯•åŸºç¡€è§„åˆ™
    base_result = service.test_filter_with_logs(base_filter, test_logs)
    results.append({"type": "base", "filter": base_filter, "result": base_result})
    
    # å°è¯•ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥
    optimizations = [
        # æ·»åŠ å­—æ®µé‡å‘½å
        base_filter + '\nmutate { rename => { "clientip" => "src_ip" } }',
        
        # æ·»åŠ å­—æ®µç±»å‹è½¬æ¢
        base_filter + '\nmutate { convert => { "response" => "integer", "bytes" => "integer" } }',
        
        # æ·»åŠ æ¡ä»¶å¤„ç†
        base_filter + '\nif [response] >= 400 { mutate { add_tag => ["error"] } }'
    ]
    
    for i, opt_filter in enumerate(optimizations):
        opt_result = service.test_filter_with_logs(opt_filter, test_logs)
        results.append({"type": f"optimization_{i+1}", "filter": opt_filter, "result": opt_result})
    
    # åˆ†ææœ€ä½³è§„åˆ™
    best_result = max(results, key=lambda x: len(x["result"].get("parsed_events", [])))
    
    return {
        "best_filter": best_result["filter"],
        "all_results": results,
        "recommendation": f"æ¨èä½¿ç”¨ {best_result['type']} è§„åˆ™"
    }
```

### ğŸ§ª **3. æ‰¹é‡æ—¥å¿—æµ‹è¯•**

```python
def batch_log_testing(service, log_samples_by_type):
    """æ‰¹é‡æµ‹è¯•ä¸åŒç±»å‹çš„æ—¥å¿—"""
    
    test_results = {}
    
    # é¢„å®šä¹‰çš„ Filter æ¨¡æ¿
    filter_templates = {
        "apache": 'grok { match => { "message" => "%{COMBINEDAPACHELOG}" } }',
        "json": 'json { source => "message" }',
        "syslog": 'grok { match => { "message" => "%{SYSLOGTIMESTAMP:timestamp} %{IPORHOST:host} %{PROG:program}: %{GREEDYDATA:log_message}" } }'
    }
    
    for log_type, log_samples in log_samples_by_type.items():
        print(f"\nğŸ§ª æµ‹è¯• {log_type} æ—¥å¿—ç±»å‹...")
        
        if log_type in filter_templates:
            filter_rule = filter_templates[log_type]
            is_json = log_type == "json"
            
            result = service.test_filter_with_logs(filter_rule, log_samples, is_json=is_json)
            
            # åˆ†æç»“æœ
            if result["success"]:
                analysis = service.analyze_parsing_effectiveness(result["parsed_events"])
                test_results[log_type] = {
                    "success": True,
                    "events_count": len(result["parsed_events"]),
                    "effectiveness": analysis["effectiveness"],
                    "fields": list(analysis["parsed_fields"].keys())
                }
            else:
                test_results[log_type] = {
                    "success": False,
                    "errors": result["errors"]
                }
        else:
            test_results[log_type] = {
                "success": False,
                "errors": [f"æœªæ‰¾åˆ° {log_type} ç±»å‹çš„ Filter æ¨¡æ¿"]
            }
    
    return test_results
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹å’Œæœ€ä½³å®è·µ

### ğŸš¨ **é”™è¯¯å¤„ç†**

```python
def robust_api_call(func, *args, **kwargs):
    """å¥å£®çš„ API è°ƒç”¨åŒ…è£…å™¨"""
    max_retries = 3
    retry_delay = 1
    
    for attempt in range(max_retries):
        try:
            return func(*args, **kwargs)
        except requests.exceptions.ConnectionError:
            if attempt < max_retries - 1:
                print(f"è¿æ¥å¤±è´¥ï¼Œ{retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                raise Exception("æœåŠ¡è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€")
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                print(f"è¯·æ±‚è¶…æ—¶ï¼Œ{retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
            else:
                raise Exception("æœåŠ¡å“åº”è¶…æ—¶")
        except Exception as e:
            raise Exception(f"API è°ƒç”¨å¼‚å¸¸: {str(e)}")
```

### â±ï¸ **æ€§èƒ½ä¼˜åŒ–**

```python
# 1. ä½¿ç”¨è¿æ¥æ± 
session = requests.Session()
adapter = requests.adapters.HTTPAdapter(pool_connections=10, pool_maxsize=20)
session.mount('http://', adapter)

# 2. è®¾ç½®åˆç†çš„è¶…æ—¶
timeout_config = {
    "save_filter": 10,    # Filter ä¿å­˜å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
    "test": 5,           # å‘é€æ—¥å¿—
    "get_results": 3,    # è·å–ç»“æœ
    "logs": 8            # è·å–æ—¥å¿—å¯èƒ½è¾ƒæ…¢
}

# 3. æ‰¹é‡å¤„ç†
def batch_send_logs(service, logs, batch_size=5):
    """æ‰¹é‡å‘é€æ—¥å¿—ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚"""
    results = []
    for i in range(0, len(logs), batch_size):
        batch = logs[i:i+batch_size]
        batch_content = '\n'.join(batch)
        result = service.send_test_log(batch_content)
        results.append(result)
        time.sleep(0.5)  # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡è½½
    return results
```

### ğŸ” **è°ƒè¯•æ”¯æŒ**

```python
import logging

# é…ç½®è°ƒè¯•æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

class DebugLogstashService(LogstashTestService):
    """å¸¦è°ƒè¯•åŠŸèƒ½çš„æœåŠ¡å®¢æˆ·ç«¯"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.debug = True
    
    def _debug_log(self, message):
        if self.debug:
            logger.debug(f"[LogstashService] {message}")
    
    def save_filter(self, filter_content):
        self._debug_log(f"ä¿å­˜ Filter: {len(filter_content)} å­—ç¬¦")
        result = super().save_filter(filter_content)
        self._debug_log(f"ä¿å­˜ç»“æœ: {result.get('message')}")
        return result
    
    def send_test_log(self, logs, is_json=False):
        self._debug_log(f"å‘é€æ—¥å¿—: {len(logs)} å­—ç¬¦, JSON={is_json}")
        result = super().send_test_log(logs, is_json)
        self._debug_log(f"å‘é€ç»“æœ: {result.get('message')}")
        return result
```

---

## ğŸ“‹ å®Œæ•´ç¤ºä¾‹è„šæœ¬

```python
#!/usr/bin/env python3
"""
å®Œæ•´çš„ AI é›†æˆç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Logstash æµ‹è¯•æœåŠ¡è¿›è¡Œè‡ªåŠ¨åŒ–æ—¥å¿—è§£ææµ‹è¯•
"""

import sys
from logstash_service import LogstashTestService

def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„æµ‹è¯•æµç¨‹"""
    
    print("ğŸ¤– AI é›†æˆæµ‹è¯•å¼€å§‹...")
    
    # åˆå§‹åŒ–æœåŠ¡
    service = LogstashTestService()
    
    # å¥åº·æ£€æŸ¥
    if not service.health_check():
        print("âŒ æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ Logstash æµ‹è¯•æœåŠ¡çŠ¶æ€")
        sys.exit(1)
    
    print("âœ… æœåŠ¡çŠ¶æ€æ­£å¸¸")
    
    # ğŸŒŸ æ¨èæµ‹è¯•æ•°æ®ï¼šä½¿ç”¨å®Œæ•´ Pipeline é…ç½®
    pipeline_test_cases = [
        {
            "name": "Apache è®¿é—®æ—¥å¿— Pipeline",
            "pipeline": """
input {
  http {
    port => 15515
    additional_codecs => { "application/json" => "json" }
  }
}

filter {
  if "apache" == [@metadata][type] {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
    mutate {
      rename => { "clientip" => "src_ip" }
      add_field => { "log_type" => "apache_access" }
    }
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@timestamp"
    }
  }
}

output {
  file {
    path => "/data/out/events.ndjson"
    codec => json_lines
  }
}
            """,
            "logs": [
                '127.0.0.1 - - [25/Dec/2023:10:00:00 +0000] "GET /index.html HTTP/1.1" 200 2326',
                '192.168.1.100 - user [25/Dec/2023:10:00:01 +0000] "POST /api/login HTTP/1.1" 401 128'
            ],
            "is_json": False
        },
        {
            "name": "JSON åº”ç”¨æ—¥å¿— Pipeline",
            "pipeline": """
input {
  http {
    port => 15515
    additional_codecs => { "application/json" => "json" }
  }
}

filter {
  if "json_app" == [@metadata][type] {
    json {
      source => "message"
    }
    if [level] {
      mutate {
        uppercase => ["level"]
      }
    }
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
  }
}

output {
  file {
    path => "/data/out/events.ndjson"
    codec => json_lines
  }
}
            """,
            "logs": [
                '{"timestamp": "2023-12-25T10:00:00Z", "level": "info", "message": "ç”¨æˆ·ç™»å½•æˆåŠŸ", "user_id": 12345}',
                '{"timestamp": "2023-12-25T10:00:01Z", "level": "error", "message": "æ•°æ®åº“è¿æ¥å¤±è´¥", "error_code": 500}'
            ],
            "is_json": True
        }
    ]
    
    # æ‰§è¡Œæµ‹è¯•
    all_results = []
    
    for test_case in pipeline_test_cases:
        print(f"\nğŸ§ª æµ‹è¯•: {test_case['name']}")
        print(f"Pipeline: {test_case['pipeline'][:100]}...")
        
        # ğŸŒŸ ä½¿ç”¨ Pipeline æµ‹è¯•æ–¹æ³•ï¼ˆæ¨èï¼‰
        result = service.test_pipeline_with_logs(
            test_case['pipeline'],
            test_case['logs'],
            is_json=test_case['is_json']
        )
        
        if result["success"]:
            print("âœ… æµ‹è¯•æˆåŠŸ")
            
            # åˆ†æç»“æœ
            analysis = service.analyze_parsing_effectiveness(result["parsed_events"])
            print(f"ğŸ“Š è§£ææ•ˆæœ: {analysis['effectiveness']}/100")
            print(f"ğŸ“‹ æå–å­—æ®µ: {', '.join(analysis['parsed_fields'].keys())}")
            
            all_results.append({
                "test_case": test_case['name'],
                "success": True,
                "analysis": analysis
            })
        else:
            print("âŒ æµ‹è¯•å¤±è´¥")
            for error in result["errors"]:
                print(f"  é”™è¯¯: {error}")
            
            all_results.append({
                "test_case": test_case['name'],
                "success": False,
                "errors": result["errors"]
            })
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "="*50)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("="*50)
    
    success_count = sum(1 for r in all_results if r["success"])
    total_count = len(all_results)
    
    print(f"æ€»æµ‹è¯•æ•°: {total_count}")
    print(f"æˆåŠŸæ•°: {success_count}")
    print(f"æˆåŠŸç‡: {(success_count/total_count)*100:.1f}%")
    
    for result in all_results:
        status = "âœ…" if result["success"] else "âŒ"
        print(f"{status} {result['test_case']}")
        
        if result["success"]:
            analysis = result["analysis"]
            print(f"   è§£ææ•ˆæœ: {analysis['effectiveness']}/100")
            print(f"   å­—æ®µæ•°é‡: {len(analysis['parsed_fields'])}")
        else:
            print(f"   é”™è¯¯: {'; '.join(result['errors'])}")
    
    print("\nğŸ‰ AI é›†æˆæµ‹è¯•å®Œæˆ!")
    print("\nğŸ’¡ **æœ€ä½³å®è·µæç¤º**:")
    print("â€¢ ä¼˜å…ˆä½¿ç”¨ service.test_pipeline_with_logs() æ–¹æ³•")
    print("â€¢ é¿å… URL ç¼–ç é—®é¢˜ï¼Œè·å¾—æ›´å¥½çš„æµ‹è¯•ä½“éªŒ") 
    print("â€¢ å¦‚éœ€ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ï¼Œè®°å¾—åœ¨ curl å‘½ä»¤ä¸­ä½¿ç”¨ --data-urlencode")

if __name__ == "__main__":
    main()
```

---

## ğŸ”— ç›¸å…³èµ„æº

- **é¡¹ç›®ä¸»æ–‡æ¡£**: [README.md](README.md)
- **æœåŠ¡å¯åŠ¨**: `./start.sh`
- **Web ç•Œé¢**: http://localhost:19000
- **API åŸºç¡€åœ°å€**: http://localhost:19000

---

## ğŸ“ æ”¯æŒå’Œåé¦ˆ

å¦‚æœ AI åœ¨è°ƒç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ï¼š

1. **æ£€æŸ¥æœåŠ¡çŠ¶æ€**: `curl http://localhost:19000/get_parsed_results`
2. **æŸ¥çœ‹æœåŠ¡æ—¥å¿—**: `docker compose logs -f`
3. **é‡å¯æœåŠ¡**: `docker compose restart`
4. **æ£€æŸ¥ç½‘ç»œè¿æ¥**: ç¡®ä¿ç«¯å£ 19000 å¯è®¿é—®

**å¸¸è§é—®é¢˜**:
- æœåŠ¡ä¸å“åº” â†’ æ£€æŸ¥ Docker å®¹å™¨çŠ¶æ€
- è§£æç»“æœä¸ºç©º â†’ æ£€æŸ¥ Filter è¯­æ³•å’Œæ—¥å¿—æ ¼å¼åŒ¹é…
- çƒ­é‡è½½å¤±è´¥ â†’ ç­‰å¾…æ›´é•¿æ—¶é—´æˆ–æ‰‹åŠ¨é‡å¯æœåŠ¡

---

*æœ¬æ–‡æ¡£ä¸“ä¸º AI ç³»ç»Ÿè®¾è®¡ï¼Œæä¾›äº†å®Œæ•´çš„è°ƒç”¨æŒ‡å—å’Œç¤ºä¾‹ä»£ç ã€‚*
